<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-12-05T17:33:34+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Xuan-Toan Dang Profile</title><subtitle>Xuan-Toan Dang&apos;s academic project</subtitle><author><name>Xuan-Toan Dang</name><email>dangxuantoan@soongsil.ac.kr</email></author><entry><title type="html">SAC Algorithm</title><link href="http://localhost:4000/posts/2025/20251206/SAC-algorithm/" rel="alternate" type="text/html" title="SAC Algorithm" /><published>2025-12-06T00:00:00+00:00</published><updated>2025-12-06T00:00:00+00:00</updated><id>http://localhost:4000/posts/2025/20251206/SAC-Algorithm-blog-1</id><content type="html" xml:base="http://localhost:4000/posts/2025/20251206/SAC-algorithm/"><![CDATA[<p>The <strong>Soft Actor‚ÄìCritic (SAC)</strong> algorithm is one of the most influential advancements in modern reinforcement learning (RL), especially in continuous control tasks. Although its origins lie in machine learning, the algorithm is deeply rooted in <em>mathematics</em>‚Äîfrom stochastic calculus to optimization theory and information entropy. This blog will walk you through the mathematical foundations and inner mechanics of SAC in a structured, intuitive way.</p>

<hr />

<h2 id="-what-is-sac">üîç What Is SAC?</h2>

<p>SAC is an <strong>off-policy, model-free, actor‚Äìcritic RL algorithm</strong> that optimizes a stochastic policy in an environment with continuous actions. Its distinguishing feature is <strong>entropy regularization</strong>, which encourages exploration and yields more stable learning.</p>

<p>SAC blends three major elements:</p>

<ol>
  <li><strong>Maximum Entropy Reinforcement Learning</strong></li>
  <li><strong>Actor‚ÄìCritic Framework</strong></li>
  <li><strong>Soft Q-Learning</strong></li>
</ol>

<hr />

<h1 id="soft-actorcritic-sac-algorithm--detailed-mathematical-blog">Soft Actor‚ÄìCritic (SAC) Algorithm ‚Äî Detailed Mathematical Blog</h1>

<h2 id="-mathematical-foundations">üìê Mathematical Foundations</h2>

<h3 id="1-maximum-entropy-objective">1. Maximum Entropy Objective</h3>

<p>Standard RL maximizes the expected sum of rewards:</p>

\[J(\pi) = \sum_{t=0}^{\infty} \mathbb{E}_{\pi}\left[ r(s_t, a_t) \right]\]

<p>SAC uses the <strong>maximum entropy objective</strong>, enhancing exploration and improving robustness:</p>

\[J_{\text{max-ent}}(\pi) = 
\sum_{t=0}^{\infty} 
\mathbb{E}_{\pi} 
\left[
    r(s_t, a_t) 
    + 
    \alpha \mathcal{H}(\pi(\cdot \mid s_t))
\right]\]

<p>where:</p>

<ul>
  <li>
\[\mathcal{H}(\pi(\cdot \mid s_t)) = -\mathbb{E}[ \log \pi(a_t \mid s_t) ]\]
  </li>
  <li>\(\alpha\) is the temperature parameter controlling the entropy‚Äìreward trade-off</li>
</ul>

<p>This objective encourages <strong>exploratory yet reward-driven</strong> policies.</p>

<hr />

<h2 id="-the-components-of-sac">üß† The Components of SAC</h2>

<p>SAC maintains <strong>three neural networks</strong>:</p>

<ol>
  <li>
    <p><strong>Two Q-functions</strong><br />
\(Q_{\theta_1}(s,a),\; Q_{\theta_2}(s,a)\)</p>
  </li>
  <li>
    <p><strong>A stochastic actor (policy)</strong><br />
\(\pi_\phi(a \mid s)\)</p>
  </li>
</ol>

<p>Using two Q-networks reduces overestimation bias.</p>

<hr />

<h2 id="-1-the-soft-q-function-loss">üî¢ 1. The Soft Q-Function Loss</h2>

<p>The target for Q-learning includes the entropy term:</p>

\[y = 
r(s_t, a_t) 
+ \gamma 
\mathbb{E}_{a_{t+1} \sim \pi}
\left[
    \min_{i=1,2} Q_{\theta_i}(s_{t+1}, a_{t+1})
    - 
    \alpha \log \pi(a_{t+1} \mid s_{t+1})
\right]\]

<p>The loss for each Q-function is:</p>

\[J_Q(\theta_i) 
= 
\mathbb{E}
\left[
    (Q_{\theta_i}(s_t,a_t) - y)^2
\right]\]

<hr />

<h2 id="-2-the-policy-actor-loss">üî¢ 2. The Policy (Actor) Loss</h2>

<p>The actor improves by minimizing:</p>

\[J_\pi(\phi)
=
\mathbb{E}_{s_t \sim \mathcal{D}}
\Bigg[
    \mathbb{E}_{a_t \sim \pi_\phi}
    \left[
        \alpha \log \pi_\phi(a_t \mid s_t)
        -
        \min_{i=1,2}
        Q_{\theta_i}(s_t, a_t)
    \right]
    \Bigg]\]

<p>This trades off:</p>

<ul>
  <li>better Q-values</li>
  <li>higher entropy (diverse actions)</li>
</ul>

<hr />

<h2 id="-entropy-temperature-adjustment-optional">üî• Entropy Temperature Adjustment (Optional)</h2>

<p>SAC can automatically adjust \(\alpha\) by minimizing:</p>

\[J(\alpha)
=
\mathbb{E}_{a_t \sim \pi}
\left[
    -\alpha
    \left(
        \log \pi(a_t \mid s_t)
        + 
        \mathcal{H}_{\text{target}}
    \right)
\right]\]

<p>This ensures the entropy stays close to a chosen target.</p>

<hr />

<h2 id="Ô∏è-algorithm-outline">üèóÔ∏è Algorithm Outline</h2>

<ol>
  <li>Initialize Q-networks, policy network, and target Q-networks</li>
  <li>
    <p>For each interaction step:</p>

    <ul>
      <li>Sample action from the stochastic policy</li>
      <li>Store transition in replay buffer</li>
    </ul>
  </li>
  <li>
    <p>For each gradient step:</p>

    <ul>
      <li>Update Q-networks using soft Bellman backup</li>
      <li>Update policy via entropy-regularized objective</li>
      <li>Update temperature ( \alpha ) if enabled</li>
      <li>Soft-update target networks</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="-why-sac-works-so-well">üìä Why SAC Works So Well</h2>

<h3 id="-stability">‚úî Stability</h3>

<p>Entropy regularization smooths updates and prevents premature convergence.</p>

<h3 id="-robust-exploration">‚úî Robust Exploration</h3>

<p>Stochasticity gives better performance in sparse or noisy reward tasks.</p>

<h3 id="-sample-efficiency">‚úî Sample Efficiency</h3>

<p>As an off-policy method, SAC reuses past experience effectively.</p>

<h3 id="-continuous-action-suitability">‚úî Continuous Action Suitability</h3>

<p>Perfect for robotics, control engineering, and physics-based environments.</p>

<hr />

<h2 id="-sac-vs-other-rl-algorithms">üßÆ SAC vs. Other RL Algorithms</h2>

<table>
  <thead>
    <tr>
      <th>Algorithm</th>
      <th>On/Off-Policy</th>
      <th>Deterministic?</th>
      <th>Key Strength</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>DDPG</td>
      <td>Off-policy</td>
      <td>Deterministic</td>
      <td>Fast but less stable</td>
    </tr>
    <tr>
      <td>TD3</td>
      <td>Off-policy</td>
      <td>Deterministic</td>
      <td>Reduced overestimation</td>
    </tr>
    <tr>
      <td>PPO</td>
      <td>On-policy</td>
      <td>Stochastic</td>
      <td>Stable but less sample-efficient</td>
    </tr>
    <tr>
      <td><strong>SAC</strong></td>
      <td><strong>Off-policy</strong></td>
      <td><strong>Stochastic</strong></td>
      <td><strong>Exploration + stability + efficiency</strong></td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="-summary">üßæ Summary</h2>

<p>The SAC algorithm is mathematically elegant and computationally effective. Its use of entropy regularization represents a shift toward more robust and exploratory learning strategies. With strong theoretical foundations and empirical results, SAC remains a state-of-the-art algorithm for continuous-control problems.</p>

<hr />]]></content><author><name>Xuan-Toan Dang</name><email>dangxuantoan@soongsil.ac.kr</email></author><category term="cool posts" /><category term="category1" /><category term="category2" /><summary type="html"><![CDATA[The Soft Actor‚ÄìCritic (SAC) algorithm is one of the most influential advancements in modern reinforcement learning (RL), especially in continuous control tasks. Although its origins lie in machine learning, the algorithm is deeply rooted in mathematics‚Äîfrom stochastic calculus to optimization theory and information entropy. This blog will walk you through the mathematical foundations and inner mechanics of SAC in a structured, intuitive way.]]></summary></entry></feed>